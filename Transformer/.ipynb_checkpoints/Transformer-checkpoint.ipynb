{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f090dd8-ec10-4651-a146-b1bfb6344bf7",
   "metadata": {},
   "source": [
    "# <a href=https://paul-hyun.github.io/transformer-01>reference blog</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "416ac394-2021-49f1-9144-36e8182dbf12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sentencepiece as spm\n",
    "\n",
    "vocab_file = \"../web-crawler/kowiki/kowiki.model\"\n",
    "vocab = spm.SentencePieceProcessor()\n",
    "vocab.load(vocab_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a82f1481-b5d1-4308-8763-93e92418ea91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' \\n문장 -> 토큰화 -> 정수 임베딩 -> 각 정수를 embedding layer를 통해 실수화시켜서 input으로\\n\\n이때, 사실상 정수 임베딩을 input으로 넣고, embedding layer를 통과시키는 형태이므로\\n실제 input dimension은 (Batch, Seq_len) 이다. (token이 벡터가 아니라 정수로 표현되기 때문에)\\nembedding layer를 거치면? (Batch, Seq_len, hidden_d)\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" \n",
    "문장 -> 토큰화 -> 정수 임베딩 -> 각 정수를 embedding layer를 통해 실수화시켜서 input으로\n",
    "\n",
    "이때, 사실상 정수 임베딩을 input으로 넣고, embedding layer를 통과시키는 형태이므로\n",
    "실제 input dimension은 (Batch, Seq_len) 이다. (token이 벡터가 아니라 정수로 표현되기 때문에)\n",
    "embedding layer를 거치면? (Batch, Seq_len, hidden_d)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c1c0170-2a15-4926-8f35-f11b24df8cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "acaaf640-73db-423f-a707-e743158cd196",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁겨울', '은', '▁추', '워', '요', '.']\n",
      "['▁감', '기', '▁조', '심', '하', '세', '요', '.']\n",
      "torch.Size([2, 8])\n",
      "tensor([[3221, 3648,  199, 4001, 3802, 3634,    0,    0],\n",
      "        [ 192, 3650,   54, 3879, 3640, 3726, 3802, 3634]])\n"
     ]
    }
   ],
   "source": [
    "# 입력 texts\n",
    "lines = [\n",
    "  \"겨울은 추워요.\",\n",
    "  \"감기 조심하세요.\"\n",
    "]\n",
    "\n",
    "# text를 tensor로 변환\n",
    "inputs = []\n",
    "for line in lines:\n",
    "  pieces = vocab.encode_as_pieces(line)\n",
    "  ids = vocab.encode_as_ids(line)\n",
    "  inputs.append(torch.tensor(ids))\n",
    "  print(pieces)\n",
    "\n",
    "# 입력 길이가 다르므로 입력 최대 길이에 맟춰 padding(0)을 추가 해 줌\n",
    "inputs = torch.nn.utils.rnn.pad_sequence(inputs, batch_first=True, padding_value=0)\n",
    "# shape\n",
    "print(inputs.size())\n",
    "# 값\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ad5bf940-b33e-4f5e-883a-128a27dd6e2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 8, 128])\n"
     ]
    }
   ],
   "source": [
    "n_vocab = len(vocab)\n",
    "d_hidn = 128\n",
    "nn_emb = nn.Embedding(n_vocab, d_hidn)\n",
    "\n",
    "input_embs = nn_emb(inputs)\n",
    "print(input_embs.size()) #정수 임베딩 된 토큰 하나하나를 벡터로 변환시켜줌 (차원이 하나 늘어남)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c616eded-801a-4c26-b451-47f597e84069",
   "metadata": {},
   "source": [
    "### PAD masking : positions 값에서 PAD 부분은 0으로 변경 후, emdedding 값 구하는 부분"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "02c31616-5c6c-4971-b1ed-140240af29ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positions before maksing: torch.Size([2, 8])\n",
      " tensor([[0, 1, 2, 3, 4, 5, 6, 7],\n",
      "        [0, 1, 2, 3, 4, 5, 6, 7]])\n",
      "\n",
      "inputs : torch.Size([2, 8])\n",
      " tensor([[3221, 3648,  199, 4001, 3802, 3634,    0,    0],\n",
      "        [ 192, 3650,   54, 3879, 3640, 3726, 3802, 3634]])\n",
      "\n",
      "pos_mask : \n",
      "tensor([[False, False, False, False, False, False,  True,  True],\n",
      "        [False, False, False, False, False, False, False, False]])\n",
      "\n",
      "positions after maksing: torch.Size([2, 8])\n",
      " tensor([[0, 1, 2, 3, 4, 5, 0, 0],\n",
      "        [0, 1, 2, 3, 4, 5, 6, 7]])\n",
      "\n",
      "pos_embs size : \n",
      " torch.Size([2, 8, 128])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'inputs_embs + pos_embs를 더하면 transformer input이 완성된다.'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn_pos = nn.Embedding(n_vocab,d_hidn)\n",
    "\n",
    "#0-7까지 만들고, 원소 반복을 통해 (2,8) dimension으로 확장\n",
    "positions = torch.arange(inputs.size(1),device = inputs.device, dtype = inputs.dtype).expand(inputs.size(0),inputs.size(1)).contiguous()\n",
    "pos_mask = inputs.eq(0) # inputs와 동일한 tensor size로 만들되, 0의 값인 부분은 True, 아니면 False로 구성\n",
    "print(f'positions before maksing: {positions.size()}\\n', positions)\n",
    "\n",
    "positions.masked_fill_(pos_mask,0) #positions와 pos_mask를 비교, positions == True인 부분을 0으로 바꾼다\n",
    "pos_embs = nn_pos(positions)\n",
    "\n",
    "print(f'\\ninputs : {inputs.size()}\\n',inputs)\n",
    "print(f'\\npos_mask : \\n{pos_mask}')\n",
    "print(f'\\npositions after maksing: {positions.size()}\\n', positions)\n",
    "print(f'\\npos_embs size : \\n',pos_embs.size())\n",
    "\n",
    "\n",
    "\"\"\"inputs_embs + pos_embs를 더하면 transformer input이 완성된다.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5189ffd3-76eb-4612-a93a-5af00d17aa50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 8, 128])\n"
     ]
    }
   ],
   "source": [
    "input_sums = input_embs + pos_embs\n",
    "print(input_sums.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "734061f0-29ee-4195-b2c5-229e23ce48a1",
   "metadata": {},
   "source": [
    "### Scaled Dot Product Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e2ebacb3-1872-4b65-9e4e-a47bca8149fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 8, 8])\n",
      "tensor([[False, False, False, False, False, False,  True,  True],\n",
      "        [False, False, False, False, False, False,  True,  True],\n",
      "        [False, False, False, False, False, False,  True,  True],\n",
      "        [False, False, False, False, False, False,  True,  True],\n",
      "        [False, False, False, False, False, False,  True,  True],\n",
      "        [False, False, False, False, False, False,  True,  True],\n",
      "        [False, False, False, False, False, False,  True,  True],\n",
      "        [False, False, False, False, False, False,  True,  True]])\n"
     ]
    }
   ],
   "source": [
    "Q = input_sums\n",
    "K = input_sums\n",
    "V = input_sums\n",
    "attn_mask = inputs.eq(0).unsqueeze(1).expand(Q.size(0), Q.size(1), K.size(1))\n",
    "print(attn_mask.size())\n",
    "print(attn_mask[0]) # batch index==0인 문장에 대해서, attention mask를 보여줌"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "63a6a6d3-4042-4af3-97c1-4916de5ceb57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 8, 128])\n",
      "torch.Size([2, 128, 8])\n",
      "torch.Size([2, 8, 8])\n",
      "tensor([[ 2.5324e+02,  1.5833e-01, -8.0440e-01, -4.6271e+00, -1.7822e+01,\n",
      "         -2.4352e+01,  1.0210e+02,  1.0210e+02],\n",
      "        [ 1.5833e-01,  2.7630e+02, -7.4676e-01,  4.5944e+00, -1.6067e+01,\n",
      "         -2.9491e+00,  2.1577e+01,  2.1577e+01],\n",
      "        [-8.0440e-01, -7.4676e-01,  2.8578e+02,  2.4337e+01,  1.2212e+01,\n",
      "         -1.4985e+01,  8.0787e-01,  8.0787e-01],\n",
      "        [-4.6271e+00,  4.5944e+00,  2.4337e+01,  2.6937e+02, -3.2513e+01,\n",
      "          5.7154e+00,  1.0898e+01,  1.0898e+01],\n",
      "        [-1.7822e+01, -1.6067e+01,  1.2212e+01, -3.2513e+01,  2.5882e+02,\n",
      "          1.2128e+01, -1.7531e+01, -1.7531e+01],\n",
      "        [-2.4352e+01, -2.9491e+00, -1.4985e+01,  5.7154e+00,  1.2128e+01,\n",
      "          2.5278e+02, -2.3176e+01, -2.3176e+01],\n",
      "        [ 1.0210e+02,  2.1577e+01,  8.0787e-01,  1.0898e+01, -1.7531e+01,\n",
      "         -2.3176e+01,  2.3646e+02,  2.3646e+02],\n",
      "        [ 1.0210e+02,  2.1577e+01,  8.0787e-01,  1.0898e+01, -1.7531e+01,\n",
      "         -2.3176e+01,  2.3646e+02,  2.3646e+02]], grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "scores = torch.matmul(Q,K.transpose(-1,-2))\n",
    "print(Q.size())\n",
    "print(K.transpose(-1,-2).size())\n",
    "\n",
    "print(scores.size())\n",
    "print(scores[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "45588656-1c5a-4ddf-ac46-1bac0ffe289c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 8, 8])\n",
      "tensor([[ 3.1655e+01,  1.9792e-02, -1.0055e-01, -5.7839e-01, -2.2278e+00,\n",
      "         -3.0439e+00,  1.2763e+01,  1.2763e+01],\n",
      "        [ 1.9792e-02,  3.4537e+01, -9.3345e-02,  5.7430e-01, -2.0084e+00,\n",
      "         -3.6864e-01,  2.6971e+00,  2.6971e+00],\n",
      "        [-1.0055e-01, -9.3345e-02,  3.5722e+01,  3.0422e+00,  1.5264e+00,\n",
      "         -1.8732e+00,  1.0098e-01,  1.0098e-01],\n",
      "        [-5.7839e-01,  5.7430e-01,  3.0422e+00,  3.3671e+01, -4.0642e+00,\n",
      "          7.1443e-01,  1.3623e+00,  1.3623e+00],\n",
      "        [-2.2278e+00, -2.0084e+00,  1.5264e+00, -4.0642e+00,  3.2352e+01,\n",
      "          1.5160e+00, -2.1914e+00, -2.1914e+00],\n",
      "        [-3.0439e+00, -3.6864e-01, -1.8732e+00,  7.1443e-01,  1.5160e+00,\n",
      "          3.1598e+01, -2.8970e+00, -2.8970e+00],\n",
      "        [ 1.2763e+01,  2.6971e+00,  1.0098e-01,  1.3623e+00, -2.1914e+00,\n",
      "         -2.8970e+00,  2.9557e+01,  2.9557e+01],\n",
      "        [ 1.2763e+01,  2.6971e+00,  1.0098e-01,  1.3623e+00, -2.1914e+00,\n",
      "         -2.8970e+00,  2.9557e+01,  2.9557e+01]], grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"k-dimension에 루트를 취한 값으로 나누기\"\"\"\n",
    "d_head = 64\n",
    "scores = scores.mul_(1/d_head**0.5)\n",
    "print(scores.size())\n",
    "print(scores[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a85e5d50-137a-41a7-b380-b50cf9c764dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 8, 8])\n",
      "tensor([[ 3.1655e+01,  1.9792e-02, -1.0055e-01, -5.7839e-01, -2.2278e+00,\n",
      "         -3.0439e+00, -1.0000e+09, -1.0000e+09],\n",
      "        [ 1.9792e-02,  3.4537e+01, -9.3345e-02,  5.7430e-01, -2.0084e+00,\n",
      "         -3.6864e-01, -1.0000e+09, -1.0000e+09],\n",
      "        [-1.0055e-01, -9.3345e-02,  3.5722e+01,  3.0422e+00,  1.5264e+00,\n",
      "         -1.8732e+00, -1.0000e+09, -1.0000e+09],\n",
      "        [-5.7839e-01,  5.7430e-01,  3.0422e+00,  3.3671e+01, -4.0642e+00,\n",
      "          7.1443e-01, -1.0000e+09, -1.0000e+09],\n",
      "        [-2.2278e+00, -2.0084e+00,  1.5264e+00, -4.0642e+00,  3.2352e+01,\n",
      "          1.5160e+00, -1.0000e+09, -1.0000e+09],\n",
      "        [-3.0439e+00, -3.6864e-01, -1.8732e+00,  7.1443e-01,  1.5160e+00,\n",
      "          3.1598e+01, -1.0000e+09, -1.0000e+09],\n",
      "        [ 1.2763e+01,  2.6971e+00,  1.0098e-01,  1.3623e+00, -2.1914e+00,\n",
      "         -2.8970e+00, -1.0000e+09, -1.0000e+09],\n",
      "        [ 1.2763e+01,  2.6971e+00,  1.0098e-01,  1.3623e+00, -2.1914e+00,\n",
      "         -2.8970e+00, -1.0000e+09, -1.0000e+09]], grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"PAD에 해당하는 부분을 -1e9로 바꿔줌으로써, PAD가 softmax 계산에 영향을 주지 못한다 \"\"\"\n",
    "scores.masked_fill_(attn_mask, -1e9)\n",
    "print(scores.size())\n",
    "print(scores[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b718b5c7-4d28-4b45-b1f8-62c1fc085c4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 8, 8])\n",
      "tensor([[1.0000e+00, 1.8237e-14, 1.6169e-14, 1.0027e-14, 1.9269e-15, 8.5191e-16,\n",
      "         0.0000e+00, 0.0000e+00],\n",
      "        [1.0214e-15, 1.0000e+00, 9.1210e-16, 1.7783e-15, 1.3439e-16, 6.9260e-16,\n",
      "         0.0000e+00, 0.0000e+00],\n",
      "        [2.7694e-16, 2.7894e-16, 1.0000e+00, 6.4159e-15, 1.4092e-15, 4.7049e-17,\n",
      "         0.0000e+00, 0.0000e+00],\n",
      "        [1.3359e-15, 4.2305e-15, 4.9909e-14, 1.0000e+00, 4.0920e-17, 4.8668e-15,\n",
      "         0.0000e+00, 0.0000e+00],\n",
      "        [9.5953e-16, 1.1949e-15, 4.0972e-14, 1.5294e-16, 1.0000e+00, 4.0546e-14,\n",
      "         0.0000e+00, 0.0000e+00],\n",
      "        [9.0188e-16, 1.3092e-14, 2.9081e-15, 3.8671e-14, 8.6201e-14, 1.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00],\n",
      "        [9.9994e-01, 4.2522e-05, 3.1705e-06, 1.1192e-05, 3.2031e-07, 1.5818e-07,\n",
      "         0.0000e+00, 0.0000e+00],\n",
      "        [9.9994e-01, 4.2522e-05, 3.1705e-06, 1.1192e-05, 3.2031e-07, 1.5818e-07,\n",
      "         0.0000e+00, 0.0000e+00]], grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "attn_prob = nn.Softmax(dim=-1)(scores)\n",
    "print(attn_prob.size())\n",
    "print(attn_prob[0])\n",
    "\"\"\"PAD 부분이 masking 되어서 값이 0이 되는 걸 확인할 수 있다\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "a428e2a2-a249-4b27-8786-0e6828e6976d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 8, 128])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nQ * K.T -> (bs, q_seq_len, q_hidn_d) * (bs, k_hidn_d, k_seq_len) -> (bs, q_seq_len, k_seq_len)\\nSoftmax((Q*V.T)/sqrt(k-dim)) * V -> (bs, q_seq_len, k_seq_len) * (bs, v_seq_len, v_hidn_d) -> (bs, )\\n'"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context = torch.matmul(attn_prob,V)\n",
    "print(context.size())\n",
    "\"\"\"Q와 동일한 shape이 구해진다\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "Q * K.T -> (bs, q_seq_len, q_hidn_d) * (bs, k_hidn_d, k_seq_len) -> (bs, q_seq_len, k_seq_len)\n",
    "Softmax((Q*V.T)/sqrt(k-dim)) * V -> (bs, q_seq_len, k_seq_len) * (bs, v_seq_len, v_hidn_d) \n",
    "                                    -> (bs, q_seq_len, v_hidn_d)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a6d2eaf-b08e-43f6-a298-804950df0cc1",
   "metadata": {},
   "source": [
    "### Scaled Dot Product Attention in Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a68eddb0-dcf8-4795-8ee9-34d7f921cd9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" scale dot product attention \"\"\"\n",
    "class ScaledDotProductAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "        self.scale = 1 / (self.config.d_head ** 0.5)\n",
    "    \n",
    "    def forward(self, Q, K, V, attn_mask):\n",
    "        # (bs, n_head, n_q_seq, n_k_seq)\n",
    "        scores = torch.matmul(Q, K.transpose(-1, -2))\n",
    "        scores = scores.mul_(self.scale)\n",
    "        scores.masked_fill_(attn_mask, -1e9)\n",
    "        # (bs, n_head, n_q_seq, n_k_seq)\n",
    "        attn_prob = nn.Softmax(dim=-1)(scores)\n",
    "        attn_prob = self.dropout(attn_prob)\n",
    "        # (bs, n_head, n_q_seq, d_v)\n",
    "        context = torch.matmul(attn_prob, V)\n",
    "        # (bs, n_head, n_q_seq, d_v), (bs, n_head, n_q_seq, n_v_seq)\n",
    "        return context, attn_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "788dbe62-ead1-4293-8517-c028eae955a4",
   "metadata": {},
   "source": [
    "### Multi-Head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "3c06eeed-bc93-4636-963e-0b49f8289f42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 8, 128])\n",
      "torch.Size([2, 8, 2, 64])\n",
      "torch.Size([2, 2, 8, 64])\n"
     ]
    }
   ],
   "source": [
    "Q = input_sums\n",
    "K = input_sums\n",
    "V = input_sums\n",
    "attn_mask = inputs.eq(0).unsqueeze(1).expand(Q.size(0), Q.size(1), K.size(1))\n",
    "\n",
    "batch_size = Q.size(0)\n",
    "n_head = 2\n",
    "\n",
    "W_Q = nn.Linear(d_hidn, n_head * d_head)\n",
    "W_K = nn.Linear(d_hidn, n_head * d_head)\n",
    "W_V = nn.Linear(d_hidn, n_head * d_head)\n",
    "\n",
    "# (bs, n_seq, n_head * d_head)\n",
    "q_s = W_Q(Q)\n",
    "print(q_s.size())\n",
    "# (bs, n_seq, n_head, d_head)\n",
    "q_s = q_s.view(batch_size, -1, n_head, d_head)\n",
    "print(q_s.size())\n",
    "# (bs, n_head, n_seq, d_head)\n",
    "q_s = q_s.transpose(1,2)\n",
    "print(q_s.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "b3204705-7df3-4648-b178-10d8d9533a68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2, 8, 64]) torch.Size([2, 2, 8, 64]) torch.Size([2, 2, 8, 64])\n"
     ]
    }
   ],
   "source": [
    "\"\"\"위의 수식을 한 줄로 나타내면,\"\"\"\n",
    "# (bs, n_head, n_seq, d_head)\n",
    "q_s = W_Q(Q).view(batch_size, -1, n_head, d_head).transpose(1,2)\n",
    "# (bs, n_head, n_seq, d_head)\n",
    "k_s = W_K(K).view(batch_size, -1, n_head, d_head).transpose(1,2)\n",
    "# (bs, n_head, n_seq, d_head)\n",
    "v_s = W_V(V).view(batch_size, -1, n_head, d_head).transpose(1,2)\n",
    "print(q_s.size(), k_s.size(), v_s.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "7c0c6f69-55a9-4cd5-ac03-bdba34a634b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 8, 8])\n",
      "torch.Size([2, 2, 8, 8])\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Attention mask도 multi-head로 변경\"\"\"\n",
    "print(attn_mask.size())\n",
    "attn_mask = attn_mask.unsqueeze(1).repeat(1, n_head, 1, 1)\n",
    "print(attn_mask.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab7cbdbc-8929-4844-9126-1798900aa3df",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_dot_attn = ScaledDotProductAttention(d_head)\n",
    "context, attn_prob = scaled_dot_attn(q_s, k_s, v_s, attn_mask)\n",
    "print(context.size())\n",
    "print(attn_prob.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad5b70f-5a24-4c31-a443-2b73eed7f532",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (bs, n_seq, n_head * d_head)\n",
    "context = context.transpose(1, 2).contiguous().view(batch_size, -1, n_head * d_head)\n",
    "print(context.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9be7266-b5d8-4ff0-981f-219635cfbde3",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear = nn.Linear(n_head * d_head, d_hidn)\n",
    "# (bs, n_seq, d_hidn)\n",
    "output = linear(context)\n",
    "print(output.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "9432dcc5-137f-4576-b833-2dda1cdffcb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" multi head attention \"\"\"\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.W_Q = nn.Linear(self.config.d_hidn, self.config.n_head * self.config.d_head)\n",
    "        self.W_K = nn.Linear(self.config.d_hidn, self.config.n_head * self.config.d_head)\n",
    "        self.W_V = nn.Linear(self.config.d_hidn, self.config.n_head * self.config.d_head)\n",
    "        self.scaled_dot_attn = ScaledDotProductAttention(self.config)\n",
    "        self.linear = nn.Linear(self.config.n_head * self.config.d_head, self.config.d_hidn)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "    \n",
    "    def forward(self, Q, K, V, attn_mask):\n",
    "        batch_size = Q.size(0)\n",
    "        # (bs, n_head, n_q_seq, d_head)\n",
    "        q_s = self.W_Q(Q).view(batch_size, -1, self.config.n_head, self.config.d_head).transpose(1,2)\n",
    "        # (bs, n_head, n_k_seq, d_head)\n",
    "        k_s = self.W_K(K).view(batch_size, -1, self.config.n_head, self.config.d_head).transpose(1,2)\n",
    "        # (bs, n_head, n_v_seq, d_head)\n",
    "        v_s = self.W_V(V).view(batch_size, -1, self.config.n_head, self.config.d_head).transpose(1,2)\n",
    "\n",
    "        # (bs, n_head, n_q_seq, n_k_seq)\n",
    "        attn_mask = attn_mask.unsqueeze(1).repeat(1, self.config.n_head, 1, 1)\n",
    "\n",
    "        # (bs, n_head, n_q_seq, d_head), (bs, n_head, n_q_seq, n_k_seq)\n",
    "        context, attn_prob = self.scaled_dot_attn(q_s, k_s, v_s, attn_mask)\n",
    "        # (bs, n_head, n_q_seq, h_head * d_head)\n",
    "        context = context.transpose(1, 2).contiguous().view(batch_size, -1, self.config.n_head * self.config.d_head)\n",
    "        # (bs, n_head, n_q_seq, e_embd)\n",
    "        output = self.linear(context)\n",
    "        output = self.dropout(output)\n",
    "        # (bs, n_q_seq, d_hidn), (bs, n_head, n_q_seq, n_k_seq)\n",
    "        return output, attn_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be9785f-ab51-45b8-872a-9cb963077d11",
   "metadata": {},
   "source": [
    "### Masked Multi-Head Attention\n",
    "attention decoder mask만 추가\n",
    "\n",
    "미래 정보를 가릴 때 사용된다. Decoder의 self attention시에 사용(encoder-decoder에선 PAD mask만 사용)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "be71b55d-8de1-4444-8c5e-0ff9769ea85c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[False, False, False, False, False, False,  True,  True],\n",
      "        [False, False, False, False, False, False,  True,  True],\n",
      "        [False, False, False, False, False, False,  True,  True],\n",
      "        [False, False, False, False, False, False,  True,  True],\n",
      "        [False, False, False, False, False, False,  True,  True],\n",
      "        [False, False, False, False, False, False,  True,  True],\n",
      "        [False, False, False, False, False, False,  True,  True],\n",
      "        [False, False, False, False, False, False,  True,  True]])\n",
      "tensor([[0, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [0, 0, 1, 1, 1, 1, 1, 1],\n",
      "        [0, 0, 0, 1, 1, 1, 1, 1],\n",
      "        [0, 0, 0, 0, 1, 1, 1, 1],\n",
      "        [0, 0, 0, 0, 0, 1, 1, 1],\n",
      "        [0, 0, 0, 0, 0, 0, 1, 1],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 1],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "tensor([[False,  True,  True,  True,  True,  True,  True,  True],\n",
      "        [False, False,  True,  True,  True,  True,  True,  True],\n",
      "        [False, False, False,  True,  True,  True,  True,  True],\n",
      "        [False, False, False, False,  True,  True,  True,  True],\n",
      "        [False, False, False, False, False,  True,  True,  True],\n",
      "        [False, False, False, False, False, False,  True,  True],\n",
      "        [False, False, False, False, False, False,  True,  True],\n",
      "        [False, False, False, False, False, False,  True,  True]])\n"
     ]
    }
   ],
   "source": [
    "def get_attn_decoder_mask(seq):\n",
    "    subsequent_mask = torch.ones_like(seq).unsqueeze(-1).expand(seq.size(0), seq.size(1), seq.size(1))\n",
    "    subsequent_mask = subsequent_mask.triu(diagonal=1) # upper triangular part of a matrix(2-D)\n",
    "    return subsequent_mask\n",
    "\n",
    "\n",
    "Q = input_sums\n",
    "K = input_sums\n",
    "V = input_sums\n",
    "\n",
    "attn_pad_mask = inputs.eq(0).unsqueeze(1).expand(Q.size(0), Q.size(1), K.size(1))\n",
    "print(attn_pad_mask[0]) # 첫 번째 문장에 대해서 출력\n",
    "attn_dec_mask = get_attn_decoder_mask(inputs)\n",
    "print(attn_dec_mask[0])\n",
    "attn_mask = torch.gt((attn_pad_mask + attn_dec_mask), 0)\n",
    "print(attn_mask[0])\n",
    "\n",
    "batch_size = Q.size(0)\n",
    "n_head = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "ac4576d5-0867-49ad-9b35-e2e04c28eea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" attention pad mask test\"\"\"\n",
    "def get_attn_pad_mask(seq_q, seq_k, i_pad):\n",
    "    print(seq_q.size())\n",
    "    batch_size, len_q = seq_q.size()\n",
    "    batch_size, len_k = seq_k.size()\n",
    "    #Key의 한 sequence에서 id를 확인해서 PAD가 존재하면, 0으로 채우고 아래로 늘린다. 그 후, batch에 대해서 진행 \n",
    "    pad_attn_mask = seq_k.data.eq(i_pad)\n",
    "    print(seq_k.data.size())\n",
    "    pad_attn_mask= pad_attn_mask.unsqueeze(1).expand(batch_size, len_q, len_k)\n",
    "    return pad_attn_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "884fcffd-b5cc-424b-9f60-69ee1e9d509d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3221, 3648,  199, 4001, 3802, 3634,    0,    0],\n",
      "        [ 192, 3650,   54, 3879, 3640, 3726, 3802, 3634]])\n",
      "torch.Size([2, 8])\n",
      "torch.Size([2, 8])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[False, False, False, False, False, False,  True,  True],\n",
       "         [False, False, False, False, False, False,  True,  True],\n",
       "         [False, False, False, False, False, False,  True,  True],\n",
       "         [False, False, False, False, False, False,  True,  True],\n",
       "         [False, False, False, False, False, False,  True,  True],\n",
       "         [False, False, False, False, False, False,  True,  True],\n",
       "         [False, False, False, False, False, False,  True,  True],\n",
       "         [False, False, False, False, False, False,  True,  True]],\n",
       "\n",
       "        [[False, False, False, False, False, False, False, False],\n",
       "         [False, False, False, False, False, False, False, False],\n",
       "         [False, False, False, False, False, False, False, False],\n",
       "         [False, False, False, False, False, False, False, False],\n",
       "         [False, False, False, False, False, False, False, False],\n",
       "         [False, False, False, False, False, False, False, False],\n",
       "         [False, False, False, False, False, False, False, False],\n",
       "         [False, False, False, False, False, False, False, False]]])"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(inputs)\n",
    "#encode-decoder에선, (Decoder inputs, Encoder inputs)로 들어간다. decoder가 query\n",
    "get_attn_pad_mask(inputs,inputs,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31218385-b138-4155-b497-43b10f46f0eb",
   "metadata": {},
   "source": [
    "### FeedForward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "285315e4-d82f-46d9-89de-77593dd74729",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" feed forward \"\"\"\n",
    "class PoswiseFeedForwardNet(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.conv1 = nn.Conv1d(in_channels=self.config.d_hidn, out_channels=self.config.d_ff, kernel_size=1)\n",
    "        self.conv2 = nn.Conv1d(in_channels=self.config.d_ff, out_channels=self.config.d_hidn, kernel_size=1)\n",
    "        self.active = F.gelu\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # (bs, d_ff, n_seq)\n",
    "        output = self.conv1(inputs.transpose(1, 2))\n",
    "        output = self.active(output)\n",
    "        # (bs, n_seq, d_hidn)\n",
    "        output = self.conv2(output).transpose(1, 2)\n",
    "        output = self.dropout(output)\n",
    "        # (bs, n_seq, d_hidn)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ac35dfb1-21c2-497c-a213-600d7059c860",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" configuration json을 읽어들이는 class \"\"\"\n",
    "class Config(dict): \n",
    "    __getattr__ = dict.__getitem__\n",
    "    __setattr__ = dict.__setitem__\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, file):\n",
    "        with open(file, 'r') as f:\n",
    "            config = json.loads(f.read())\n",
    "            return Config(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "4bba0217-6c19-410d-8a2d-d12b203f1b94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_enc_vocab': 8007, 'n_dec_vocab': 8007, 'n_enc_seq': 256, 'n_dec_seq': 256, 'n_layer': 6, 'd_hidn': 256, 'i_pad': 0, 'd_ff': 1024, 'n_head': 4, 'd_head': 64, 'dropout': 0.1, 'layer_norm_epsilon': 1e-12}\n"
     ]
    }
   ],
   "source": [
    "config = Config({\n",
    "    \"n_enc_vocab\": len(vocab),\n",
    "    \"n_dec_vocab\": len(vocab),\n",
    "    \"n_enc_seq\": 256,\n",
    "    \"n_dec_seq\": 256,\n",
    "    \"n_layer\": 6,\n",
    "    \"d_hidn\": 256,\n",
    "    \"i_pad\": 0,\n",
    "    \"d_ff\": 1024,\n",
    "    \"n_head\": 4,\n",
    "    \"d_head\": 64,\n",
    "    \"dropout\": 0.1,\n",
    "    \"layer_norm_epsilon\": 1e-12\n",
    "})\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "f9c4f671-d4bc-4f25-b2d1-b6f61e1f0f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" attention pad mask \"\"\"\n",
    "def get_attn_pad_mask(seq_q, seq_k, i_pad):\n",
    "    batch_size, len_q = seq_q.size()\n",
    "    batch_size, len_k = seq_k.size()\n",
    "     \n",
    "    pad_attn_mask = seq_k.data.eq(i_pad)\n",
    "    pad_attn_mask= pad_attn_mask.unsqueeze(1).expand(batch_size, len_q, len_k)\n",
    "    return pad_attn_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "6e25ffc6-f8d4-4641-8651-ab975eb0d576",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" encoder layer \"\"\"\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.self_attn = MultiHeadAttention(self.config)\n",
    "        self.layer_norm1 = nn.LayerNorm(self.config.d_hidn, eps=self.config.layer_norm_epsilon)\n",
    "        self.pos_ffn = PoswiseFeedForwardNet(self.config)\n",
    "        self.layer_norm2 = nn.LayerNorm(self.config.d_hidn, eps=self.config.layer_norm_epsilon)\n",
    "    \n",
    "    def forward(self, inputs, attn_mask):\n",
    "        # (bs, n_enc_seq, d_hidn), (bs, n_head, n_enc_seq, n_enc_seq)\n",
    "        att_outputs, attn_prob = self.self_attn(inputs, inputs, inputs, attn_mask)\n",
    "        att_outputs = self.layer_norm1(inputs + att_outputs)\n",
    "        # (bs, n_enc_seq, d_hidn)\n",
    "        ffn_outputs = self.pos_ffn(att_outputs)\n",
    "        ffn_outputs = self.layer_norm2(ffn_outputs + att_outputs)\n",
    "        # (bs, n_enc_seq, d_hidn), (bs, n_head, n_enc_seq, n_enc_seq)\n",
    "        return ffn_outputs, attn_prob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "746f8c70-a437-499a-ab5c-a9f666c3ed5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" encoder \"\"\"\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.enc_emb = nn.Embedding(self.config.n_enc_vocab, self.config.d_hidn)\n",
    "        self.pos_emb = nn.Embedding(self.config.n_enc_vocab, self.config.d_hidn)\n",
    "\n",
    "        self.layers = nn.ModuleList([EncoderLayer(self.config) for _ in range(self.config.n_layer)])\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        # inputs -> (bs, seq_len)\n",
    "        positions = torch.arange(inputs.size(1), device=inputs.device, dtype=inputs.dtype).expand(inputs.size(0), inputs.size(1)).contiguous() + 1\n",
    "        pos_mask = inputs.eq(self.config.i_pad)\n",
    "        positions.masked_fill_(pos_mask, 0)\n",
    "\n",
    "        # (bs, n_enc_seq, d_hidn)\n",
    "        outputs = self.enc_emb(inputs) + self.pos_emb(positions)\n",
    "\n",
    "        # (bs, n_enc_seq, n_enc_seq)\n",
    "        attn_mask = get_attn_pad_mask(inputs, inputs, self.config.i_pad)\n",
    "\n",
    "        attn_probs = []\n",
    "        for layer in self.layers:\n",
    "            # (bs, n_enc_seq, d_hidn), (bs, n_head, n_enc_seq, n_enc_seq)\n",
    "            outputs, attn_prob = layer(outputs, attn_mask)\n",
    "            attn_probs.append(attn_prob)\n",
    "        # (bs, n_enc_seq, d_hidn), [(bs, n_head, n_enc_seq, n_enc_seq)]\n",
    "        return outputs, attn_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "d61dc676-fdb2-4aba-9f42-671ad2864c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" decoder layer \"\"\"\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.self_attn = MultiHeadAttention(self.config)\n",
    "        self.layer_norm1 = nn.LayerNorm(self.config.d_hidn, eps=self.config.layer_norm_epsilon)\n",
    "        self.dec_enc_attn = MultiHeadAttention(self.config)\n",
    "        self.layer_norm2 = nn.LayerNorm(self.config.d_hidn, eps=self.config.layer_norm_epsilon)\n",
    "        self.pos_ffn = PoswiseFeedForwardNet(self.config)\n",
    "        self.layer_norm3 = nn.LayerNorm(self.config.d_hidn, eps=self.config.layer_norm_epsilon)\n",
    "    \n",
    "    def forward(self, dec_inputs, enc_outputs, self_attn_mask, dec_enc_attn_mask):\n",
    "        # (bs, n_dec_seq, d_hidn), (bs, n_head, n_dec_seq, n_dec_seq)\n",
    "        self_att_outputs, self_attn_prob = self.self_attn(dec_inputs, dec_inputs, dec_inputs, self_attn_mask)\n",
    "        self_att_outputs = self.layer_norm1(dec_inputs + self_att_outputs)\n",
    "        # (bs, n_dec_seq, d_hidn), (bs, n_head, n_dec_seq, n_enc_seq)\n",
    "        dec_enc_att_outputs, dec_enc_attn_prob = self.dec_enc_attn(self_att_outputs, enc_outputs, enc_outputs, dec_enc_attn_mask)\n",
    "        dec_enc_att_outputs = self.layer_norm2(self_att_outputs + dec_enc_att_outputs)\n",
    "        # (bs, n_dec_seq, d_hidn)\n",
    "        ffn_outputs = self.pos_ffn(dec_enc_att_outputs)\n",
    "        ffn_outputs = self.layer_norm3(dec_enc_att_outputs + ffn_outputs)\n",
    "        # (bs, n_dec_seq, d_hidn), (bs, n_head, n_dec_seq, n_dec_seq), (bs, n_head, n_dec_seq, n_enc_seq)\n",
    "        return ffn_outputs, self_attn_prob, dec_enc_attn_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "7368f735-ae5b-4b82-a30b-163cc260ceb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" decoder \"\"\"\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.dec_emb = nn.Embedding(self.config.n_dec_vocab, self.config.d_hidn)\n",
    "        self.pos_emb = nn.Embedding(self.config.n_dec_vocab, self.config.d_hidn)\n",
    "\n",
    "        self.layers = nn.ModuleList([DecoderLayer(self.config) for _ in range(self.config.n_layer)])\n",
    "    \n",
    "    def forward(self, dec_inputs, enc_inputs, enc_outputs):\n",
    "        positions = torch.arange(dec_inputs.size(1), device=dec_inputs.device, dtype=dec_inputs.dtype).expand(dec_inputs.size(0), dec_inputs.size(1)).contiguous() + 1\n",
    "        pos_mask = dec_inputs.eq(self.config.i_pad)\n",
    "        positions.masked_fill_(pos_mask, 0)\n",
    "    \n",
    "        # (bs, n_dec_seq, d_hidn)\n",
    "        dec_outputs = self.dec_emb(dec_inputs) + self.pos_emb(positions)\n",
    "\n",
    "        # (bs, n_dec_seq, n_dec_seq)\n",
    "        dec_attn_pad_mask = get_attn_pad_mask(dec_inputs, dec_inputs, self.config.i_pad)\n",
    "        # (bs, n_dec_seq, n_dec_seq)\n",
    "        dec_attn_decoder_mask = get_attn_decoder_mask(dec_inputs)\n",
    "        # (bs, n_dec_seq, n_dec_seq)\n",
    "        dec_self_attn_mask = torch.gt((dec_attn_pad_mask + dec_attn_decoder_mask), 0)\n",
    "        # (bs, n_dec_seq, n_enc_seq)\n",
    "        dec_enc_attn_mask = get_attn_pad_mask(dec_inputs, enc_inputs, self.config.i_pad)\n",
    "\n",
    "        self_attn_probs, dec_enc_attn_probs = [], []\n",
    "        for layer in self.layers:\n",
    "            # (bs, n_dec_seq, d_hidn), (bs, n_dec_seq, n_dec_seq), (bs, n_dec_seq, n_enc_seq)\n",
    "            dec_outputs, self_attn_prob, dec_enc_attn_prob = layer(dec_outputs, enc_outputs, dec_self_attn_mask, dec_enc_attn_mask)\n",
    "            self_attn_probs.append(self_attn_prob)\n",
    "            dec_enc_attn_probs.append(dec_enc_attn_prob)\n",
    "        # (bs, n_dec_seq, d_hidn), [(bs, n_dec_seq, n_dec_seq)], [(bs, n_dec_seq, n_enc_seq)]S\n",
    "        return dec_outputs, self_attn_probs, dec_enc_attn_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "7bd0592b-415b-4d91-9f39-aaa01945b0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" transformer \"\"\"\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.encoder = Encoder(self.config)\n",
    "        self.decoder = Decoder(self.config)\n",
    "    \n",
    "    def forward(self, enc_inputs, dec_inputs):\n",
    "        # (bs, n_enc_seq, d_hidn), [(bs, n_head, n_enc_seq, n_enc_seq)]\n",
    "        enc_outputs, enc_self_attn_probs = self.encoder(enc_inputs)\n",
    "        # (bs, n_seq, d_hidn), [(bs, n_head, n_dec_seq, n_dec_seq)], [(bs, n_head, n_dec_seq, n_enc_seq)]\n",
    "        dec_outputs, dec_self_attn_probs, dec_enc_attn_probs = self.decoder(dec_inputs, enc_inputs, enc_outputs)\n",
    "        # (bs, n_dec_seq, n_dec_vocab), [(bs, n_head, n_enc_seq, n_enc_seq)], [(bs, n_head, n_dec_seq, n_dec_seq)], [(bs, n_head, n_dec_seq, n_enc_seq)]\n",
    "        return dec_outputs, enc_self_attn_probs, dec_self_attn_probs, dec_enc_attn_probs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9fe4ce3-b193-4d65-8c9b-ad45b37e859a",
   "metadata": {},
   "source": [
    "# Naver 영화리뷰 감정분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "df655a29-6cc2-40a8-b7fa-3230b4f4043e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-08-15 18:21:02--  https://raw.githubusercontent.com/e9t/nsmc/master/ratings_train.txt\n",
      "raw.githubusercontent.com (raw.githubusercontent.com)을(를) 해석하는 중... 185.199.109.133, 185.199.111.133, 185.199.108.133, ...\n",
      "접속 raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... 접속됨.\n",
      "HTTP 요청을 전송했습니다. 응답을 기다리는 중입니다... 200 OK\n",
      "길이: 14628807 (14M) [text/plain]\n",
      "다음 위치에 저장: `ratings_train.txt'\n",
      "\n",
      "ratings_train.txt   100%[===================>]  13.95M  --.-KB/s    / 0.1s     \n",
      "\n",
      "2023-08-15 18:21:03 (110 MB/s) - `ratings_train.txt' 저장됨 [14628807/14628807]\n",
      "\n",
      "--2023-08-15 18:21:03--  https://raw.githubusercontent.com/e9t/nsmc/master/ratings_test.txt\n",
      "raw.githubusercontent.com (raw.githubusercontent.com)을(를) 해석하는 중... 185.199.110.133, 185.199.108.133, 185.199.111.133, ...\n",
      "접속 raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... 접속됨.\n",
      "HTTP 요청을 전송했습니다. 응답을 기다리는 중입니다... 200 OK\n",
      "길이: 4893335 (4.7M) [text/plain]\n",
      "다음 위치에 저장: `ratings_test.txt'\n",
      "\n",
      "ratings_test.txt    100%[===================>]   4.67M  --.-KB/s    / 0.04s    \n",
      "\n",
      "2023-08-15 18:21:04 (105 MB/s) - `ratings_test.txt' 저장됨 [4893335/4893335]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/e9t/nsmc/master/ratings_train.txt\n",
    "!wget https://raw.githubusercontent.com/e9t/nsmc/master/ratings_test.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "383a1569-5124-49ff-bc21-c3c656d8abb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# vocab loading\n",
    "vocab_file = \"../web-crawler/kowiki/kowiki.model\"\n",
    "vocab = spm.SentencePieceProcessor()\n",
    "vocab.load(vocab_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "62ff3e00-7c26-4783-a49f-6bcd25314b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" train data 준비 \"\"\"\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "def prepare_train(vocab, infile, outfile):\n",
    "    df = pd.read_csv(infile, sep=\"\\t\", engine=\"python\")\n",
    "    with open(outfile, \"w\") as f:\n",
    "        for index, row in df.iterrows():\n",
    "            document = row[\"document\"]\n",
    "            if type(document) != str:\n",
    "                continue\n",
    "            instance = { \"id\": row[\"id\"], \"doc\": vocab.encode_as_pieces(document), \"label\": row[\"label\"] }\n",
    "            f.write(json.dumps(instance))\n",
    "            f.write(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "8428c928-92e4-40d1-b068-4e50832f53f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "prepare_train(vocab, \"./ratings_train.txt\", \"./ratings_train.json\")\n",
    "prepare_train(vocab, \"./ratings_test.txt\", \"./ratings_test.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "e2e27905-d882-403a-9034-81f79bb906c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" naver movie classfication \"\"\"\n",
    "class MovieClassification(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.transformer = Transformer(self.config)\n",
    "        self.projection = nn.Linear(self.config.d_hidn, self.config.n_output, bias=False)\n",
    "    \n",
    "    def forward(self, enc_inputs, dec_inputs):\n",
    "        # (bs, n_dec_seq, d_hidn), [(bs, n_head, n_enc_seq, n_enc_seq)], [(bs, n_head, n_dec_seq, n_dec_seq)], [(bs, n_head, n_dec_seq, n_enc_seq)]\n",
    "        dec_outputs, enc_self_attn_probs, dec_self_attn_probs, dec_enc_attn_probs = self.transformer(enc_inputs, dec_inputs)\n",
    "        # (bs, d_hidn)\n",
    "        dec_outputs, _ = torch.max(dec_outputs, dim=1)\n",
    "        # (bs, n_output)\n",
    "        logits = self.projection(dec_outputs)\n",
    "        # (bs, n_output), [(bs, n_head, n_enc_seq, n_enc_seq)], [(bs, n_head, n_dec_seq, n_dec_seq)], [(bs, n_head, n_dec_seq, n_enc_seq)]\n",
    "        return logits, enc_self_attn_probs, dec_self_attn_probs, dec_enc_attn_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "76b01454-5937-40ac-bc64-4b1cbbdb455e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" 영화 분류 데이터셋 \"\"\"\n",
    "class MovieDataSet(torch.utils.data.Dataset):\n",
    "    def __init__(self, vocab, infile):\n",
    "        self.vocab = vocab\n",
    "        self.labels = []\n",
    "        self.sentences = []\n",
    "\n",
    "        line_cnt = 0\n",
    "        with open(infile, \"r\") as f:\n",
    "            for line in f:\n",
    "                line_cnt += 1\n",
    "\n",
    "        with open(infile, \"r\") as f:\n",
    "            for i, line in enumerate(tqdm(f, total=line_cnt, desc=f\"Loading {infile}\", unit=\" lines\")):\n",
    "                data = json.loads(line)\n",
    "                self.labels.append(data[\"label\"])\n",
    "                self.sentences.append([vocab.piece_to_id(p) for p in data[\"doc\"]])\n",
    "    \n",
    "    def __len__(self):\n",
    "        assert len(self.labels) == len(self.sentences)\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, item):\n",
    "        return (torch.tensor(self.labels[item]),\n",
    "                torch.tensor(self.sentences[item]),\n",
    "                torch.tensor([self.vocab.piece_to_id(\"[BOS]\")]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "c63d0fdf-4d79-4dad-a43d-855e075abb30",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" movie data collate_fn \"\"\"\n",
    "def movie_collate_fn(inputs):\n",
    "    labels, enc_inputs, dec_inputs = list(zip(*inputs))\n",
    "\n",
    "    enc_inputs = torch.nn.utils.rnn.pad_sequence(enc_inputs, batch_first=True, padding_value=0)\n",
    "    dec_inputs = torch.nn.utils.rnn.pad_sequence(dec_inputs, batch_first=True, padding_value=0)\n",
    "\n",
    "    batch = [\n",
    "        torch.stack(labels, dim=0),\n",
    "        enc_inputs,\n",
    "        dec_inputs,\n",
    "    ]\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "dcdcd0b8-0b14-434f-b255-647be619f4e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading ./ratings_train.json: 100%|██████████| 149995/149995 [00:03<00:00, 43965.40 lines/s]\n",
      "Loading ./ratings_test.json: 100%|██████████| 49997/49997 [00:01<00:00, 40008.75 lines/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "batch_size = 128\n",
    "train_dataset = MovieDataSet(vocab, \"./ratings_train.json\")\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=movie_collate_fn)\n",
    "test_dataset = MovieDataSet(vocab, \"./ratings_test.json\")\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=movie_collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "74286f69-b9a6-466f-9a91-f2443ba7ddbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" 모델 epoch 평가 \"\"\"\n",
    "def eval_epoch(config, model, data_loader):\n",
    "    matchs = []\n",
    "    model.eval()\n",
    "\n",
    "    n_word_total = 0\n",
    "    n_correct_total = 0\n",
    "    with tqdm(total=len(data_loader), desc=f\"Valid\") as pbar:\n",
    "        for i, value in enumerate(data_loader):\n",
    "            labels, enc_inputs, dec_inputs = map(lambda v: v.to(config.device), value)\n",
    "\n",
    "            outputs = model(enc_inputs, dec_inputs)\n",
    "            logits = outputs[0]\n",
    "            _, indices = logits.max(1)\n",
    "\n",
    "            match = torch.eq(indices, labels).detach()\n",
    "            matchs.extend(match.cpu())\n",
    "            accuracy = np.sum(matchs) / len(matchs) if 0 < len(matchs) else 0\n",
    "\n",
    "            pbar.update(1)\n",
    "            pbar.set_postfix_str(f\"Acc: {accuracy:.3f}\")\n",
    "    return np.sum(matchs) / len(matchs) if 0 < len(matchs) else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "29d68f2f-a403-4075-9319-b5d0124c2c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" 모델 epoch 학습 \"\"\"\n",
    "def train_epoch(config, epoch, model, criterion, optimizer, train_loader):\n",
    "    losses = []\n",
    "    model.train()\n",
    "\n",
    "    with tqdm(total=len(train_loader), desc=f\"Train {epoch}\") as pbar:\n",
    "        for i, value in enumerate(train_loader):\n",
    "            labels, enc_inputs, dec_inputs = map(lambda v: v.to(config.device), value)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(enc_inputs, dec_inputs)\n",
    "            logits = outputs[0]\n",
    "\n",
    "            loss = criterion(logits, labels)\n",
    "            loss_val = loss.item()\n",
    "            losses.append(loss_val)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            pbar.update(1)\n",
    "            pbar.set_postfix_str(f\"Loss: {loss_val:.3f} ({np.mean(losses):.3f})\")\n",
    "    return np.mean(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "b588b785-21e5-452e-9433-3b3e385a3509",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_enc_vocab': 8007, 'n_dec_vocab': 8007, 'n_enc_seq': 256, 'n_dec_seq': 256, 'n_layer': 6, 'd_hidn': 256, 'i_pad': 0, 'd_ff': 1024, 'n_head': 4, 'd_head': 64, 'dropout': 0.1, 'layer_norm_epsilon': 1e-12, 'device': device(type='cuda'), 'n_output': 2}\n"
     ]
    }
   ],
   "source": [
    "config.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "config.n_output = 2\n",
    "print(config)\n",
    "\n",
    "learning_rate = 5e-5\n",
    "n_epoch = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "a08e68b5-41fe-4691-8511-32b79db4ac74",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train 0: 100%|██████████| 1172/1172 [02:59<00:00,  6.53it/s, Loss: 0.490 (0.512)]\n",
      "Valid: 100%|██████████| 391/391 [01:08<00:00,  5.73it/s, Acc: 0.787]\n",
      "Train 1: 100%|██████████| 1172/1172 [02:39<00:00,  7.36it/s, Loss: 0.480 (0.427)]\n",
      "Valid: 100%|██████████| 391/391 [01:08<00:00,  5.73it/s, Acc: 0.804]\n",
      "Train 2: 100%|██████████| 1172/1172 [02:40<00:00,  7.31it/s, Loss: 0.504 (0.395)]\n",
      "Valid: 100%|██████████| 391/391 [01:07<00:00,  5.77it/s, Acc: 0.813]\n",
      "Train 3: 100%|██████████| 1172/1172 [02:38<00:00,  7.38it/s, Loss: 0.357 (0.371)]\n",
      "Valid: 100%|██████████| 391/391 [01:07<00:00,  5.75it/s, Acc: 0.815]\n",
      "Train 4: 100%|██████████| 1172/1172 [02:39<00:00,  7.37it/s, Loss: 0.452 (0.348)]\n",
      "Valid: 100%|██████████| 391/391 [01:08<00:00,  5.75it/s, Acc: 0.817]\n",
      "Train 5: 100%|██████████| 1172/1172 [02:37<00:00,  7.44it/s, Loss: 0.279 (0.325)]\n",
      "Valid: 100%|██████████| 391/391 [01:08<00:00,  5.75it/s, Acc: 0.817]\n",
      "Train 6: 100%|██████████| 1172/1172 [02:39<00:00,  7.37it/s, Loss: 0.390 (0.299)]\n",
      "Valid: 100%|██████████| 391/391 [01:08<00:00,  5.70it/s, Acc: 0.812]\n",
      "Train 7: 100%|██████████| 1172/1172 [02:39<00:00,  7.35it/s, Loss: 0.232 (0.275)]\n",
      "Valid: 100%|██████████| 391/391 [01:08<00:00,  5.75it/s, Acc: 0.819]\n",
      "Train 8: 100%|██████████| 1172/1172 [02:39<00:00,  7.33it/s, Loss: 0.293 (0.248)]\n",
      "Valid: 100%|██████████| 391/391 [01:08<00:00,  5.72it/s, Acc: 0.817]\n",
      "Train 9: 100%|██████████| 1172/1172 [02:38<00:00,  7.39it/s, Loss: 0.363 (0.223)]\n",
      "Valid: 100%|██████████| 391/391 [01:08<00:00,  5.75it/s, Acc: 0.818]\n"
     ]
    }
   ],
   "source": [
    "model = MovieClassification(config)\n",
    "model.to(config.device)\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "losses, scores = [], []\n",
    "for epoch in range(n_epoch):\n",
    "    loss = train_epoch(config, epoch, model, criterion, optimizer, train_loader)\n",
    "    score = eval_epoch(config, model, test_loader)\n",
    "\n",
    "    losses.append(loss)\n",
    "    scores.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "b04166b6-1222-4076-84d3-d312167274ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.511938</td>\n",
       "      <td>0.787367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.426636</td>\n",
       "      <td>0.804228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.395374</td>\n",
       "      <td>0.812509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.370861</td>\n",
       "      <td>0.815209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.347965</td>\n",
       "      <td>0.817469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.324703</td>\n",
       "      <td>0.817269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.298839</td>\n",
       "      <td>0.811509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.274868</td>\n",
       "      <td>0.818909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.247942</td>\n",
       "      <td>0.816769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.222867</td>\n",
       "      <td>0.817669</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       loss     score\n",
       "0  0.511938  0.787367\n",
       "1  0.426636  0.804228\n",
       "2  0.395374  0.812509\n",
       "3  0.370861  0.815209\n",
       "4  0.347965  0.817469\n",
       "5  0.324703  0.817269\n",
       "6  0.298839  0.811509\n",
       "7  0.274868  0.818909\n",
       "8  0.247942  0.816769\n",
       "9  0.222867  0.817669"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 864x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# table\n",
    "data = {\n",
    "    \"loss\": losses,\n",
    "    \"score\": scores\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "display(df)\n",
    "\n",
    "# graph\n",
    "plt.figure(figsize=[12, 4])\n",
    "plt.plot(losses, label=\"loss\")\n",
    "plt.plot(scores, label=\"score\")\n",
    "plt.legend()\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Value')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c839da49-75f6-438a-89a0-a6830c7aac71",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
